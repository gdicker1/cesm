<?xml version="1.0"?>
<config_machines>
  <machine MACH="casper">
    <DESC>NCAR SGI platform with GPUs, os is Linux, 36 pes/node, batch system is SLURM</DESC>
    <NODENAME_REGEX>casper*</NODENAME_REGEX>
    <!-- MPT sometimes timesout at model start time, the next two lines cause
                      case_run.py to detect the timeout and retry FORCE_SPARE_NODES times -->
    <MPIRUN_RETRY_REGEX>MPT: xmpi_net_accept_timeo/accept() timeout</MPIRUN_RETRY_REGEX>
    <MPIRUN_RETRY_COUNT>10</MPIRUN_RETRY_COUNT>
    <OS>LINUX</OS>
    <COMPILERS>pgi,intel,gnu</COMPILERS>
    <MPILIBS compiler="pgi" >openmpi,mpt</MPILIBS>
    <MPILIBS compiler="intel" >mpt,openmpi</MPILIBS>
    <MPILIBS compiler="gnu" >openmpi</MPILIBS>
    <CIME_OUTPUT_ROOT>/glade/scratch/$USER</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>$ENV{CESMDATAROOT}/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/glade/p/cgd/tss/CTSM_datm_forcing_data</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>$ENV{CESMDATAROOT}/cesm_baselines</BASELINE_ROOT>
    <CCSM_CPRNC>$ENV{CESMDATAROOT}/tools/cime/tools/cprnc/cprnc.cheyenne</CCSM_CPRNC>
    <GMAKE_J>8</GMAKE_J>
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>cseg</SUPPORTED_BY>
    <!-- have not seen any performance benefit in smt -->
    <MAX_TASKS_PER_NODE>36</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>36</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
    <mpirun mpilib="default">
      <executable>mpirun</executable>
      <arguments>
        <arg name="anum_tasks"> -np {{ total_tasks }}</arg>
      </arguments>
    </mpirun>
    <mpirun mpilib="openmpi">
      <executable>mpirun</executable>
      <arguments>
        <arg name="anum_tasks"> -np {{ total_tasks }}</arg>
      </arguments>
    </mpirun>
    <mpirun mpilib="mpt">
      <executable>mpiexec_mpt</executable>
      <arguments>
        <!-- for mpt/2.19 the -p needs to preceed -np -->
        <arg name="labelstdout">-p "%g:"</arg>
        <arg name="num_tasks"> -np {{ total_tasks }}</arg>
        <!-- the omplace argument needs to be last -->
        <arg name="zthreadplacement"> omplace -tm open64 </arg>
      </arguments>
    </mpirun>
    <mpirun mpilib="mpt" queue="share">
      <executable>mpirun `hostname`</executable>
      <arguments>
        <arg name="anum_tasks"> -np {{ total_tasks }}</arg>
        <!-- the omplace argument needs to be last -->
        <arg name="zthreadplacement"> omplace -tm open64 </arg>
      </arguments>
    </mpirun>
    <module_system type="module">
      <init_path lang="perl">/glade/u/apps/dav/opt/lmod/7.7.29/init/perl</init_path>
      <init_path lang="python">/glade/u/apps/dav/opt/lmod/7.7.29/init/env_modules_python.py</init_path>
      <init_path lang="csh">/glade/u/apps/dav/opt/lmod/7.7.29/init/csh</init_path>
      <init_path lang="sh">/glade/u/apps/dav/opt/lmod/7.7.29/init/sh</init_path>
      <cmd_path lang="perl">/glade/u/apps/dav/opt/lmod/7.7.29/libexec/lmod perl</cmd_path>
      <cmd_path lang="python">/glade/u/apps/dav/opt/lmod/7.7.29/libexec/lmod python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <modules>
        <command name="purge"/>
        <command name="load">ncarenv/1.2</command>
      </modules>
      <modules compiler="pgi">
        <command name="use">/glade/work/cponder/SHARE/Modules/Latest</command>
        <command name="use">/glade/work/cponder/SHARE/Modules/Legacy</command>
        <command name="use">--append /glade/work/cponder/SHARE/Modules/Bundles</command>
        <command name="use">--append /glade/work/cponder/SHARE/Modules/PrgEnv/PGI+OpenMPI/2019-04-30</command>
        <command name="load">PrgEnv/PGI+OpenMPI/2019-04-30</command>
        <command name="load">pgi/19.4</command>
      </modules>
      <modules compiler="intel">
        <command name="load">intel/17.0.1</command>
        <command name="load">esmf_libs</command>
        <command name="load">mkl</command>
      </modules>
      <modules compiler="intel" mpilib="!mpi-serial" DEBUG="TRUE" comp_interface="mct">
        <command name="load">esmf-7.1.0r-defio-mpi-g</command>
      </modules>
      <modules compiler="intel" mpilib="!mpi-serial" DEBUG="FALSE" comp_interface="mct">
        <command name="load">esmf-7.1.0r-defio-mpi-O</command>
      </modules>
      <modules compiler="intel" mpilib="mpi-serial" DEBUG="TRUE" comp_interface="mct">
        <command name="load">esmf-7.1.0r-ncdfio-uni-g</command>
      </modules>
      <modules compiler="intel" mpilib="mpi-serial" DEBUG="FALSE" comp_interface="mct">
        <command name="load">esmf-7.1.0r-ncdfio-uni-O</command>
      </modules>
      <modules compiler="gnu">
        <command name="load">gnu/7.3.0</command>
        <command name="load">openblas/0.2.20</command>
      </modules>
      <modules mpilib="openmpi" compiler="pgi">
        <command name="load">openmpi/3.1.4</command>
        <command name="load">netcdf-c/4.7.0</command>
        <command name="load">netcdf-f/4.4.5</command>
        <command name="load">pnetcdf/1.11.0</command>
        <command name="load">pio/2_4_2</command>
      </modules>
      <modules mpilib="mpt" compiler="gnu">
        <command name="load">mpt/2.16</command>
        <command name="load">netcdf-mpi/4.6.1</command>
      </modules>
      <modules mpilib="mpt" compiler="intel">
        <command name="load">mpt/2.19</command>
        <command name="load">netcdf-mpi/4.6.1</command>
        <command name="load">pnetcdf/1.11.0</command>
      </modules>
      <modules mpilib="mpt" compiler="pgi">
        <command name="load">mpt/2.15f</command>
        <command name="load">netcdf-mpi/4.5.0</command>
        <command name="load">pnetcdf/1.9.0</command>
      </modules>
      <modules>
        <command name="load">ncarcompilers/0.4.1</command>
      </modules>
      <modules compiler="gnu" mpilib="mpi-serial">
        <command name="load">netcdf/4.6.1</command>
      </modules>
      <modules compiler="pgi" mpilib="mpi-serial">
        <command name="load">netcdf/4.4.1.1</command>
      </modules>
      <modules compiler="intel" mpilib="mpi-serial">
        <command name="load">netcdf/4.5.0</command>
      </modules>
    </module_system>
    <environment_variables>
      <env name="OMP_STACKSIZE">256M</env>
      <env name="TMPDIR">/glade/scratch/$USER</env>
      <env name="MPI_TYPE_DEPTH">16</env>
      <env name="MPI_IB_CONGESTED">1</env>
      <env name="MPI_USE_ARRAY"/>
    </environment_variables>
    <environment_variables compiler="pgi">
      <env name="NETCDF_C_PATH">/glade/work/cponder/SHARE/Utils/NetCDF-C/4.7.0/PGI-19.4_OpenMPI-3.1.4_PNetCDF-1.11.1_HDF5-1.10.5</env>
      <env name="NETCDF_FORTRAN_PATH">/glade/work/cponder/SHARE/Utils/NetCDF-F/4.4.5/PGI-19.4_OpenMPI-3.1.4_NetCDF-C-4.7.0</env>
    </environment_variables>
    <environment_variables comp_interface="nuopc">
      <env name="ESMFMKFILE">/glade/u/home/dunlap/ESMF-INSTALL/8.0.0bs29/lib/libg/Linux.intel.64.mpt.default/esmf.mk</env>
      <env name="ESMF_RUNTIME_PROFILE">ON</env>
      <env name="ESMF_RUNTIME_PROFILE_OUTPUT">SUMMARY</env>
      <env name="UGCSINPUTPATH">/glade/work/dunlap/FV3GFS/benchmark-20181016/</env>
    </environment_variables>
    <environment_variables unit_testing="true">
      <env name="MPI_USE_ARRAY">false</env>
    </environment_variables>
    <resource_limits>
      <resource name="RLIMIT_STACK">-1</resource>
    </resource_limits>
  </machine>

</config_machines>
