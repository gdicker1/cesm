<?xml version="1.0"?>
<config_machines version="2.0">
  <machine MACH="casper">
    <DESC>NCAR GPU platform, os is Linux, 32 pes/node, batch system is slurm</DESC>
    <NODENAME_REGEX>casper*</NODENAME_REGEX>
    <!-- MPT sometimes timesout at model start time, the next two lines cause
                      case_run.py to detect the timeout and retry FORCE_SPARE_NODES times -->
    <MPIRUN_RETRY_REGEX>openmpi: Launcher network accept (MPI_LAUNCH_TIMEOUT) timed out</MPIRUN_RETRY_REGEX>
    <MPIRUN_RETRY_COUNT>10</MPIRUN_RETRY_COUNT>
    <OS>LINUX</OS>
    <COMPILERS>pgi</COMPILERS>
    <MPILIBS compiler="pgi" >openmpi</MPILIBS>
    <CIME_OUTPUT_ROOT>/glade/scratch/$USER</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>$ENV{CESMDATAROOT}/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>/glade/p/cgd/tss/CTSM_datm_forcing_data</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>$ENV{CESMDATAROOT}/cesm_baselines</BASELINE_ROOT>
    <GMAKE_J>8</GMAKE_J>
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>cseg</SUPPORTED_BY>
    <!-- have not seen any performance benefit in smt -->
    <MAX_TASKS_PER_NODE>32</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>32</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
    <mpirun mpilib="default">
      <executable>mpirun</executable>
      <arguments>
        <arg name="labelstdout">-p "%g:"</arg>
        <arg name="num_tasks"> -np {{ total_tasks }}</arg>
      </arguments>
    </mpirun>
    <mpirun mpilib="openmpi">
      <executable>mpirun</executable>
      <arguments>
        <arg name="anum_tasks"> -np {{ total_tasks }}</arg>
      </arguments>
    </mpirun>
    <mpirun mpilib="default" unit_testing="true">
      <!-- The only place we can build and run the unit tests is on casper's
                                 shared nodes. However, running mpi jobs on the shared nodes currently
           requires some workarounds; these workarounds are implemented here -->
      <executable>/glade/u/apps/dav/opt/openmpi/3.1.4/pgi/19.9/bin/mpirun $ENV{UNIT_TEST_HOST} -np 1 </executable>
    </mpirun>
    <module_system type="module">
      <init_path lang="perl">/glade/u/apps/dav/opt/lmod/7.7.29/init/perl</init_path>
      <init_path lang="python">/glade/u/apps/dav/opt/lmod/7.7.29/init/env_modules_python.py</init_path>
      <init_path lang="sh">/glade/u/apps/dav/opt/lmod/7.7.29/init/sh</init_path>
      <init_path lang="csh">/glade/u/apps/dav/opt/lmod/7.7.29/init/csh</init_path>
      <cmd_path lang="perl">/glade/u/apps/dav/opt/lmod/7.7.29/libexec/lmod perl</cmd_path>
      <cmd_path lang="python">/glade/u/apps/dav/opt/lmod/7.7.29/libexec/lmod python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <modules>
        <command name="purge"/>
        <command name="load">ncarenv/1.3</command>
      </modules>
      <modules compiler="pgi">
        <command name="use">/glade/work/cponder/SHARE/Modules/Latest</command>
        <command name="use">/glade/work/cponder/SHARE/Modules/Legacy</command>
        <command name="use">--append /glade/work/cponder/SHARE/Modules/Bundles</command>
        <command name="use">--append /glade/work/cponder/SHARE/Modules/PGI+OpenMPI/2019-11-08</command>
        <command name="unuse">/glade/u/apps/dav/modulefiles/default/openmpi/3.1.2/intel/17.0.1</command>
        <command name="unuse">/glade/u/apps/dav/modulefiles/default/intel/17.0.1</command>
        <command name="unuse">/glade/u/apps/dav/modulefiles/default/compilers</command>
        <command name="load">PrgEnv/PGI+OpenMPI/2019-11-08</command>
        <command name="load">pgi/19.10</command>
      </modules>
      <modules mpilib="openmpi" compiler="pgi">
        <command name="load">openmpi/3.1.4</command>
        <command name="load">pio/2_4_4</command>
      </modules>
    </module_system>
    <environment_variables>
      <env name="MODULEPATH">/glade/work/cponder/SHARE/Modules/Legacy:/glade/work/cponder/SHARE/Modules/Latest:/glade/u/apps/dav/modulefiles/default/idep:/glade/work/cponder/SHARE/Modules/Bundles:/glade/work/cponder/SHARE/Modules/PrgEnv/PGI+OpenMPI/2019-11-08</env>
      <env name="OMP_STACKSIZE">256M</env>
      <env name="TMPDIR">/glade/scratch/$USER</env>
      <env name="MPI_TYPE_DEPTH">16</env>
      <env name="CESMDATAROOT">/glade/p/cesmdata/cseg</env>
      <env name="MPI_IB_CONGESTED">1</env>
      <env name="MPI_USE_ARRAY"/>
    </environment_variables>
    <environment_variables compiler="pgi">
      <env name="NETCDF_C_PATH">/glade/work/cponder/SHARE/Utils/NetCDF-C/4.7.2/PGI-19.10_OpenMPI-3.1.4_PNetCDF-1.12.0_HDF5-1.10.5</env>
      <env name="NETCDF_FORTRAN_PATH">/glade/work/cponder/SHARE/Utils/NetCDF-F/4.5.2/PGI-19.10_OpenMPI-3.1.4_NetCDF-C-4.7.2</env>
    </environment_variables>
    <environment_variables unit_testing="true">
      <env name="MPI_USE_ARRAY">false</env>
    </environment_variables>
    <environment_variables queue="share">
      <env name="TMPDIR">/glade/scratch/$USER</env>
      <env name="MPI_USE_ARRAY">false</env>
    </environment_variables>
    <resource_limits>
      <resource name="RLIMIT_STACK">-1</resource>
    </resource_limits>
  </machine>

</config_machines>
